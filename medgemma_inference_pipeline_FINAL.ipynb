{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "A100",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "rbObPzSWJOcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/medgemma-4b-it\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/medgemma-4b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "QrdwrWRXJOc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ],
      "metadata": {
        "id": "ik1291usJOc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "KgPYHhO6JOc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"google/medgemma-4b-it\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "pipe(text=messages)"
      ],
      "metadata": {
        "id": "PCE0v-y8JOc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oxcjSsDiLZ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/vlm files/archive.zip\" -d \"/content/covid_dataset\""
      ],
      "metadata": {
        "id": "MbNzYnDELgUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment, running this may bill your account above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, using the **auto** provider setting (automatically selects an available inference provider)."
      ],
      "metadata": {
        "id": "tMxezoh6JOc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"google/medgemma-4b-it\", device_map=\"auto\")\n",
        "\n",
        "image_paths = image_files[:20]\n",
        "results = []\n",
        "\n",
        "for path in image_paths:\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Does this chest x-ray display signs of pneumonia?\"}\n",
        "        ]\n",
        "    }]\n",
        "\n",
        "    try:\n",
        "        output = pipe(text=messages)\n",
        "        response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "    except Exception as e:\n",
        "        response = f\"ERROR: {str(e)}\"\n",
        "\n",
        "    results.append((os.path.basename(path), response))\n",
        "\n",
        "# Save CSV\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results, columns=[\"Image\", \"Prediction\"])\n",
        "df.to_csv(\"covid_inference_20.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"covid_inference_20.csv\")"
      ],
      "metadata": {
        "id": "sTia-STsJOc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "#working version for all files\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"google/medgemma-4b-it\", device_map=\"auto\")\n",
        "output_path = \"covid_inference_all.csv\"\n",
        "\n",
        "# loads results\n",
        "if os.path.exists(output_path):\n",
        "    df_existing = pd.read_csv(output_path)\n",
        "    processed_files = set(df_existing[\"Image\"])\n",
        "    results = df_existing.values.tolist()\n",
        "    print(f\"Resuming from {len(results)} already processed images.\")\n",
        "else:\n",
        "    processed_files = set()\n",
        "    results = []\n",
        "\n",
        "# runs inference\n",
        "for path in tqdm(image_files, desc=\"Processing images\"):\n",
        "    filename = os.path.basename(path)\n",
        "    if filename in processed_files:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": \"Does this chest x-ray display signs of pneumonia?\"}\n",
        "            ]\n",
        "        }]\n",
        "        output = pipe(text=messages)\n",
        "        response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "    except Exception as e:\n",
        "        response = f\"ERROR: {str(e)}\"\n",
        "\n",
        "    results.append((filename, response))\n",
        "    pd.DataFrame(results, columns=[\"Image\", \"Prediction\"]).to_csv(output_path, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_path)\n"
      ],
      "metadata": {
        "id": "h_LElvNYTMnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}